[PREPROCESSING]
# Data preprocessing configuration

# Train-test split ratio
test_size = 0.2

# Random state for reproducibility
random_state = 42

# Imputation method for missing values
# Options: median, mean, mode, forward_fill, backward_fill
imputation_method = median

# Feature engineering
# Enable/disable creation of new features
engineer_features = True

# Scaling method for numerical features
# Options: standard (z-score), minmax, robust
scaling_method = standard

# Outlier handling
# Options: none, iqr, zscore
handle_outliers = none


[FEATURES_TO_ENGINEER]
# Feature engineering configuration

# Cost per unit: Total_cost / Product_weight
cost_per_unit = True

# Cost per km: Total_cost / Distance
cost_per_km = True

# Lead time efficiency: Distance / Delivery_Time
lead_time_efficiency = True

# Order size category: Binned Product_weight
order_size_category = True

# Order size bins for categorization (in kg)
order_size_bins = [0, 100, 500, 1000]
order_size_labels = [small, medium, large, extra_large]


[COLUMNS]
# Column names and types configuration

# Numerical columns (for normalization)
numerical_columns = Total_cost, Product_weight, Distance, Delivery_Time

# Categorical columns (for encoding)
categorical_columns = Mode_of_Shipment, Product_importance, Gender

# Target variable
target_column = Reached_on_Time_Y_N

# ID columns (to exclude from processing)
id_columns = Customer_ID, Order_ID


[OUTPUT]
# Output file configuration

# Save processed data
save_processed_data = True
processed_data_path = processed_data.csv

# Save scaler object
save_scaler = True
scaler_path = scaler.pkl

# Save encoders
save_encoders = True
encoders_path = encoders.pkl

# Generate correlation heatmap
generate_heatmap = True
heatmap_path = feature_correlation_heatmap.png
heatmap_dpi = 300
heatmap_figure_size = [14, 10]


[VALIDATION]
# Data validation rules

# Minimum rows required
min_rows = 100

# Maximum missing value percentage per column
max_missing_percent = 50

# Check for duplicates
check_duplicates = True

# Validate data types
validate_types = True


[LOGGING]
# Logging configuration

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level = INFO

# Log to file
log_to_file = False
log_file_path = preprocessing.log

# Console output
console_output = True


[ADVANCED]
# Advanced preprocessing options

# Handle multicollinearity
handle_multicollinearity = False

# Variance threshold for feature selection
variance_threshold = 0.0

# Number of features to keep (0 = keep all)
n_features = 0

# Use GPU acceleration if available
use_gpu = False
